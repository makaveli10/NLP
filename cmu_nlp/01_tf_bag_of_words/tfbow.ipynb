{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "from models import tfBoW\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "# read corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "def read_dataset(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            tag, words = line.lower().strip().split(\" ||| \")\n",
    "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 16582\n",
      "No of tags: 5\n"
     ]
    }
   ],
   "source": [
    "train = list(read_dataset(\"../data/classes/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/classes/train.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)\n",
    "print(\"Vocab size: {}\".format(nwords))\n",
    "print(\"No of tags: {}\".format(ntags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = tfBoW(nwords, ntags)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def train_step(words, tag):\n",
    "    with tf.GradientTape() as tape:\n",
    "        scores = model(words)\n",
    "        loss = loss_fn(tag, scores)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss/sent=329.7098, time=24.58s\n",
      "epoch 0: test acc=0.1996\n",
      "Epoch 1: train loss/sent=329.7070, time=26.55s\n",
      "epoch 1: test acc=0.1948\n",
      "Epoch 2: train loss/sent=329.7128, time=29.41s\n",
      "epoch 2: test acc=0.1966\n",
      "Epoch 3: train loss/sent=329.7158, time=26.76s\n",
      "epoch 3: test acc=0.2101\n",
      "Epoch 4: train loss/sent=329.7070, time=29.29s\n",
      "epoch 4: test acc=0.2024\n",
      "Epoch 5: train loss/sent=329.7039, time=21.09s\n",
      "epoch 5: test acc=0.1969\n",
      "Epoch 6: train loss/sent=329.7064, time=21.04s\n",
      "epoch 6: test acc=0.1951\n",
      "Epoch 7: train loss/sent=329.7057, time=21.41s\n",
      "epoch 7: test acc=0.2037\n",
      "Epoch 8: train loss/sent=329.7081, time=25.65s\n",
      "epoch 8: test acc=0.1898\n",
      "Epoch 9: train loss/sent=329.7061, time=23.86s\n",
      "epoch 9: test acc=0.1825\n",
      "Epoch 10: train loss/sent=329.7057, time=24.55s\n",
      "epoch 10: test acc=0.1997\n",
      "Epoch 11: train loss/sent=329.7071, time=23.67s\n",
      "epoch 11: test acc=0.1910\n",
      "Epoch 12: train loss/sent=329.7060, time=27.20s\n",
      "epoch 12: test acc=0.1875\n",
      "Epoch 13: train loss/sent=329.7058, time=25.40s\n",
      "epoch 13: test acc=0.2065\n",
      "Epoch 14: train loss/sent=329.7057, time=26.69s\n",
      "epoch 14: test acc=0.2114\n",
      "Epoch 15: train loss/sent=329.7055, time=27.66s\n",
      "epoch 15: test acc=0.1952\n",
      "Epoch 16: train loss/sent=329.7063, time=27.51s\n",
      "epoch 16: test acc=0.2049\n",
      "Epoch 17: train loss/sent=329.7058, time=24.32s\n",
      "epoch 17: test acc=0.2122\n",
      "Epoch 18: train loss/sent=329.7057, time=44.53s\n",
      "epoch 18: test acc=0.1859\n",
      "Epoch 19: train loss/sent=329.7045, time=32.98s\n",
      "epoch 19: test acc=0.1955\n",
      "Epoch 20: train loss/sent=329.7047, time=34.59s\n",
      "epoch 20: test acc=0.2031\n",
      "Epoch 21: train loss/sent=329.7058, time=25.11s\n",
      "epoch 21: test acc=0.2097\n",
      "Epoch 22: train loss/sent=329.7068, time=25.48s\n",
      "epoch 22: test acc=0.1963\n",
      "Epoch 23: train loss/sent=329.7047, time=29.11s\n",
      "epoch 23: test acc=0.2066\n",
      "Epoch 24: train loss/sent=329.7055, time=24.44s\n",
      "epoch 24: test acc=0.2185\n",
      "Epoch 25: train loss/sent=329.7047, time=47.56s\n",
      "epoch 25: test acc=0.2062\n",
      "Epoch 26: train loss/sent=329.7055, time=47.90s\n",
      "epoch 26: test acc=0.1979\n",
      "Epoch 27: train loss/sent=329.7052, time=46.52s\n",
      "epoch 27: test acc=0.2130\n",
      "Epoch 28: train loss/sent=329.7048, time=47.15s\n",
      "epoch 28: test acc=0.2132\n",
      "Epoch 29: train loss/sent=329.7052, time=48.40s\n",
      "epoch 29: test acc=0.1898\n",
      "Epoch 30: train loss/sent=329.7045, time=46.89s\n",
      "epoch 30: test acc=0.1960\n",
      "Epoch 31: train loss/sent=329.7041, time=47.12s\n",
      "epoch 31: test acc=0.1962\n",
      "Epoch 32: train loss/sent=329.7049, time=47.21s\n",
      "epoch 32: test acc=0.1879\n",
      "Epoch 33: train loss/sent=329.7049, time=47.36s\n",
      "epoch 33: test acc=0.2048\n",
      "Epoch 34: train loss/sent=329.7056, time=46.81s\n",
      "epoch 34: test acc=0.2005\n",
      "Epoch 35: train loss/sent=329.7041, time=47.77s\n",
      "epoch 35: test acc=0.1914\n",
      "Epoch 36: train loss/sent=329.7049, time=47.33s\n",
      "epoch 36: test acc=0.1896\n",
      "Epoch 37: train loss/sent=329.7040, time=46.83s\n",
      "epoch 37: test acc=0.1960\n",
      "Epoch 38: train loss/sent=329.7046, time=48.29s\n",
      "epoch 38: test acc=0.2029\n",
      "Epoch 39: train loss/sent=329.7045, time=47.62s\n",
      "epoch 39: test acc=0.1999\n",
      "Epoch 40: train loss/sent=329.7048, time=47.08s\n",
      "epoch 40: test acc=0.1966\n",
      "Epoch 41: train loss/sent=329.7046, time=48.08s\n",
      "epoch 41: test acc=0.2107\n",
      "Epoch 42: train loss/sent=329.7039, time=48.07s\n",
      "epoch 42: test acc=0.1962\n",
      "Epoch 43: train loss/sent=329.7063, time=48.09s\n",
      "epoch 43: test acc=0.2013\n",
      "Epoch 44: train loss/sent=329.7060, time=46.91s\n",
      "epoch 44: test acc=0.1962\n",
      "Epoch 45: train loss/sent=329.7053, time=46.62s\n",
      "epoch 45: test acc=0.1874\n",
      "Epoch 46: train loss/sent=329.7046, time=47.14s\n",
      "epoch 46: test acc=0.1875\n",
      "Epoch 47: train loss/sent=329.7043, time=47.66s\n",
      "epoch 47: test acc=0.1864\n",
      "Epoch 48: train loss/sent=329.7037, time=24.30s\n",
      "epoch 48: test acc=0.2013\n",
      "Epoch 49: train loss/sent=329.7048, time=27.91s\n",
      "epoch 49: test acc=0.1963\n",
      "Epoch 50: train loss/sent=329.7057, time=24.74s\n",
      "epoch 50: test acc=0.2038\n",
      "Epoch 51: train loss/sent=329.7047, time=24.63s\n",
      "epoch 51: test acc=0.1908\n",
      "Epoch 52: train loss/sent=329.7048, time=25.91s\n",
      "epoch 52: test acc=0.1964\n",
      "Epoch 53: train loss/sent=329.7037, time=23.57s\n",
      "epoch 53: test acc=0.1991\n",
      "Epoch 54: train loss/sent=329.7039, time=49.71s\n",
      "epoch 54: test acc=0.2066\n",
      "Epoch 55: train loss/sent=329.7045, time=47.58s\n",
      "epoch 55: test acc=0.1973\n",
      "Epoch 56: train loss/sent=329.7047, time=47.82s\n",
      "epoch 56: test acc=0.2131\n",
      "Epoch 57: train loss/sent=329.7056, time=47.88s\n",
      "epoch 57: test acc=0.2024\n",
      "Epoch 58: train loss/sent=329.7048, time=47.96s\n",
      "epoch 58: test acc=0.2075\n",
      "Epoch 59: train loss/sent=329.7040, time=48.13s\n",
      "epoch 59: test acc=0.1859\n",
      "Epoch 60: train loss/sent=329.7046, time=34.46s\n",
      "epoch 60: test acc=0.2248\n",
      "Epoch 61: train loss/sent=329.7047, time=20.25s\n",
      "epoch 61: test acc=0.2001\n",
      "Epoch 62: train loss/sent=329.7046, time=20.08s\n",
      "epoch 62: test acc=0.2223\n",
      "Epoch 63: train loss/sent=329.7049, time=19.77s\n",
      "epoch 63: test acc=0.1993\n",
      "Epoch 64: train loss/sent=329.7032, time=20.18s\n",
      "epoch 64: test acc=0.2007\n",
      "Epoch 65: train loss/sent=329.7047, time=19.70s\n",
      "epoch 65: test acc=0.1972\n",
      "Epoch 66: train loss/sent=329.7048, time=1011.37s\n",
      "epoch 66: test acc=0.1887\n",
      "Epoch 67: train loss/sent=329.7045, time=31.07s\n",
      "epoch 67: test acc=0.2028\n",
      "Epoch 68: train loss/sent=329.7054, time=29.95s\n",
      "epoch 68: test acc=0.1919\n",
      "Epoch 69: train loss/sent=329.7045, time=32.72s\n",
      "epoch 69: test acc=0.1951\n",
      "Epoch 70: train loss/sent=329.7052, time=30.18s\n",
      "epoch 70: test acc=0.1959\n",
      "Epoch 71: train loss/sent=329.7047, time=43.53s\n",
      "epoch 71: test acc=0.1973\n",
      "Epoch 72: train loss/sent=329.7054, time=46.24s\n",
      "epoch 72: test acc=0.1993\n",
      "Epoch 73: train loss/sent=329.7053, time=44.96s\n",
      "epoch 73: test acc=0.1919\n",
      "Epoch 74: train loss/sent=329.7049, time=45.21s\n",
      "epoch 74: test acc=0.2022\n",
      "Epoch 75: train loss/sent=329.7049, time=48.42s\n",
      "epoch 75: test acc=0.1880\n",
      "Epoch 76: train loss/sent=329.7056, time=48.23s\n",
      "epoch 76: test acc=0.2022\n",
      "Epoch 77: train loss/sent=329.7037, time=48.77s\n",
      "epoch 77: test acc=0.2040\n",
      "Epoch 78: train loss/sent=329.7052, time=48.68s\n",
      "epoch 78: test acc=0.2051\n",
      "Epoch 79: train loss/sent=329.7037, time=48.92s\n",
      "epoch 79: test acc=0.2061\n",
      "Epoch 80: train loss/sent=329.7046, time=48.32s\n",
      "epoch 80: test acc=0.1932\n",
      "Epoch 81: train loss/sent=329.7044, time=47.97s\n",
      "epoch 81: test acc=0.1977\n",
      "Epoch 82: train loss/sent=329.7055, time=48.42s\n",
      "epoch 82: test acc=0.2210\n",
      "Epoch 83: train loss/sent=329.7052, time=48.36s\n",
      "epoch 83: test acc=0.2120\n",
      "Epoch 84: train loss/sent=329.7039, time=48.58s\n",
      "epoch 84: test acc=0.2021\n",
      "Epoch 85: train loss/sent=329.7047, time=48.75s\n",
      "epoch 85: test acc=0.1949\n",
      "Epoch 86: train loss/sent=329.7049, time=48.95s\n",
      "epoch 86: test acc=0.1978\n",
      "Epoch 87: train loss/sent=329.7046, time=48.63s\n",
      "epoch 87: test acc=0.1908\n",
      "Epoch 88: train loss/sent=329.7046, time=48.43s\n",
      "epoch 88: test acc=0.2013\n",
      "Epoch 89: train loss/sent=329.7049, time=48.53s\n",
      "epoch 89: test acc=0.1914\n",
      "Epoch 90: train loss/sent=329.7039, time=48.63s\n",
      "epoch 90: test acc=0.1855\n",
      "Epoch 91: train loss/sent=329.7037, time=48.40s\n",
      "epoch 91: test acc=0.1967\n",
      "Epoch 92: train loss/sent=329.7046, time=48.61s\n",
      "epoch 92: test acc=0.1977\n",
      "Epoch 93: train loss/sent=329.7039, time=48.52s\n",
      "epoch 93: test acc=0.2047\n",
      "Epoch 94: train loss/sent=329.7046, time=48.04s\n",
      "epoch 94: test acc=0.1905\n",
      "Epoch 95: train loss/sent=329.7030, time=47.90s\n",
      "epoch 95: test acc=0.2088\n",
      "Epoch 96: train loss/sent=329.7056, time=47.78s\n",
      "epoch 96: test acc=0.1853\n",
      "Epoch 97: train loss/sent=329.7040, time=48.42s\n",
      "epoch 97: test acc=0.1814\n",
      "Epoch 98: train loss/sent=329.7063, time=48.16s\n",
      "epoch 98: test acc=0.1882\n",
      "Epoch 99: train loss/sent=329.7046, time=48.56s\n",
      "epoch 99: test acc=0.1841\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        words = tf.constant(words, dtype=tf.float32)\n",
    "        tag = tf.constant([tag], dtype=tf.float32)\n",
    "        loss = train_step(words, tag)\n",
    "#     loss_ = train_step(words, tag)\n",
    "    train_loss += loss.numpy()\n",
    "    print(\"Epoch %r: train loss/sent=%.4f, time=%.2fs\" % (\n",
    "                epoch, train_loss/len(train), time.time()-start_time))\n",
    "    test_correct = 0\n",
    "    for words, tag in dev:\n",
    "        words = tf.constant(words, dtype=tf.float32)\n",
    "        tag = tf.constant([tag], dtype=tf.float32)\n",
    "        scores = model(words)[0].numpy()\n",
    "        predict = np.argmax(scores)\n",
    "        if predict == tag:\n",
    "            test_correct += 1\n",
    "    print(\"epoch %r: test acc=%.4f\" % (epoch, test_correct/len(dev)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
