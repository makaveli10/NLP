{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length of the n-gram\n",
    "N = 2 # The length of the n-gram\n",
    "EMB_SIZE = 128 # The size of the embedding\n",
    "HID_SIZE = 128 # The size of the hidden layer\n",
    "\n",
    "\n",
    "# Functions to read in the corpus\n",
    "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
    "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
    "#       data we would have to do pre-processing and consider how to choose\n",
    "#       unknown words, etc.\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield [w2i[x] for x in line.strip().split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN_LM(tf.keras.Model):\n",
    "    def __init__(self, nwords, emb_size, hid_size, num_hist, dropout):\n",
    "        super(FFN_LM, self).__init__()\n",
    "        self.embedding = layers.Embedding(nwords, emb_size)\n",
    "        self.fnn = tf.keras.Sequential()\n",
    "        self.fnn.add(layers.Dense(hid_size, activation='tanh'))\n",
    "        self.fnn.add(layers.Dropout(dropout))\n",
    "        self.fnn.add(layers.Dense(nwords))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 3D Tensor of size [batch_size x num_hist x emb_size]\n",
    "        emb_out = self.embedding(inputs) \n",
    "        print(emb_out)\n",
    "        # 2D Tensor of size [batch_size x (num_hist * emb_size)]\n",
    "        emb_view =  tf.reshape(emb_out, [tf.shape(emb_out)[0], -1])\n",
    "        # 2D Tensor of size [batch_size x nwords]\n",
    "        out = self.fnn(emb_view)\n",
    "        print(\"out shape: {}\".format(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and the optimizer\n",
    "model = FFN_LM(nwords=nwords, emb_size=EMB_SIZE, hid_size=HID_SIZE, num_hist=N, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_variables(words):\n",
    "    var = tf.constant(words)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to calculate scores for one value\n",
    "def calc_score_of_histories(words):\n",
    "    # This will change from a list of histories, to a pytorch Variable whose data type is LongTensor\n",
    "    words_var = convert_to_variables(words)\n",
    "    logits = model(words_var)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def step(words_var, all_targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(words_var)\n",
    "        loss = loss_fn(all_targets, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def valid_step(words_var, all_targets):\n",
    "    logits = model(words_var)\n",
    "    loss = loss_fn(all_targets, logits)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss value for the entire sentence\n",
    "def calc_sent_loss(sent):\n",
    "    # The initial history is equal to end of sentence symbols\n",
    "    hist = [S] * N\n",
    "    # Step through the sentence, including the end of sentence token\n",
    "    all_histories = []\n",
    "    all_targets = []\n",
    "    for next_word in sent + [S]:\n",
    "        all_histories.append(list(hist))\n",
    "        all_targets.append(next_word)\n",
    "        hist = hist[1:] + [next_word]\n",
    "    words_var = convert_to_variables(all_histories)\n",
    "    all_targets = convert_to_variables(all_targets)\n",
    "    loss = step(words_var, all_targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss value for the entire sentence\n",
    "def calc_valid_sent_loss(sent):\n",
    "    # The initial history is equal to end of sentence symbols\n",
    "    hist = [S] * N\n",
    "    # Step through the sentence, including the end of sentence token\n",
    "    all_histories = []\n",
    "    all_targets = []\n",
    "    for next_word in sent + [S]:\n",
    "        all_histories.append(list(hist))\n",
    "        all_targets.append(next_word)\n",
    "        hist = hist[1:] + [next_word]\n",
    "    words_var = convert_to_variables(all_histories)\n",
    "    all_targets = convert_to_variables(all_targets)\n",
    "    loss = valid_step(words_var, all_targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "# Generate a sentence\n",
    "def generate_sent():\n",
    "    hist = [S] * N\n",
    "    sent = []\n",
    "    while True:\n",
    "        logits = calc_score_of_histories([hist])\n",
    "        prob = tf.keras.activations.softmax(logits)\n",
    "        sampled_indices = tf.random.categorical(prob[0], num_samples=1)\n",
    "        sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "        print(sampled_indices)\n",
    "#         next_word = prob.multinomial().data[0,0]\n",
    "#         if next_word == S or len(sent) == MAX_LEN:\n",
    "#             break\n",
    "#         sent.append(next_word)\n",
    "#         hist = hist[1:] + [next_word]\n",
    "#     return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 9.210657119750977\n",
      "train_loss: 18.420509338378906\n",
      "train_loss: 27.628236770629883\n",
      "train_loss: 36.83583641052246\n",
      "train_loss: 46.02820301055908\n",
      "iter 0: train loss/word=0.6575, ppl=1.9300 (word/sec=622.28)\n",
      "Tensor(\"ffn_lm/embedding/embedding_lookup/Identity:0\", shape=(15, 2, 128), dtype=float32)\n",
      "out shape: Tensor(\"ffn_lm/sequential/dense_1/BiasAdd:0\", shape=(15, 10000), dtype=float32)\n",
      "Tensor(\"ffn_lm/embedding/embedding_lookup/Identity:0\", shape=(28, 2, 128), dtype=float32)\n",
      "out shape: Tensor(\"ffn_lm/sequential/dense_1/BiasAdd:0\", shape=(28, 10000), dtype=float32)\n",
      "Tensor(\"ffn_lm/embedding/embedding_lookup/Identity:0\", shape=(None, 2, 128), dtype=float32)\n",
      "out shape: Tensor(\"ffn_lm/sequential/dense_1/BiasAdd:0\", shape=(None, 10000), dtype=float32)\n",
      "iter 0: dev loss/word=0.4512, ppl=1.5702 (word/sec=286.29)\n",
      "tf.Tensor(\n",
      "[[[ 4.86431606e-02 -5.19695692e-02 -2.46172789e-02 -2.95909010e-02\n",
      "   -2.83480864e-02  6.29220670e-03  1.63709875e-02  1.57496106e-04\n",
      "    3.40735018e-02  7.74413347e-05  3.43121551e-02 -3.84196043e-02\n",
      "   -2.35492773e-02  2.85127554e-02 -3.16552818e-02 -1.08073000e-02\n",
      "   -3.88244763e-02 -1.14600849e-03  2.30145622e-02  4.13022973e-02\n",
      "    1.76746177e-03 -5.08970283e-02 -4.01302874e-02  1.16549265e-02\n",
      "   -1.11821378e-02  2.37596426e-02 -2.28581447e-02 -2.33152397e-02\n",
      "   -3.47695164e-02  1.18928179e-02 -3.59416381e-02  4.23572119e-03\n",
      "    7.77659472e-04  4.21420597e-02 -2.92235799e-02  3.53773162e-02\n",
      "   -5.15819676e-02 -1.78848859e-05  3.08536645e-02 -2.48512644e-02\n",
      "   -3.15888971e-02  2.15091426e-02  1.81848686e-02 -1.50000295e-02\n",
      "    3.44591215e-02 -4.56937589e-02 -2.42894031e-02  5.35657890e-02\n",
      "   -2.44036876e-02 -3.82275619e-02  1.90482084e-02  5.90201886e-03\n",
      "    1.05041191e-02  2.45351903e-02  1.03156762e-02  3.62097248e-02\n",
      "   -1.98933501e-02 -2.15351768e-02 -4.98245470e-02  1.81549985e-03\n",
      "    1.95592530e-02 -1.58324093e-02  1.12186875e-02  2.00506933e-02\n",
      "   -7.49164261e-03 -1.56808496e-02 -2.87320167e-02 -2.89484710e-02\n",
      "   -2.14090906e-02  1.87886413e-02  7.61801610e-04  4.56782198e-03\n",
      "   -1.86193790e-02 -8.91930889e-03 -2.44628675e-02 -3.62036601e-02\n",
      "   -2.51631904e-02  3.09791435e-02  2.31383764e-03  1.33656282e-02\n",
      "    4.46513183e-02  2.73046680e-02 -1.62797943e-02 -5.25099821e-02\n",
      "   -5.22785820e-03 -4.74010520e-02 -3.65665928e-02 -4.87782136e-02\n",
      "   -2.05028560e-02  1.72270406e-02 -2.78102588e-02  6.10805443e-03\n",
      "    3.79271917e-02 -3.33886556e-02 -2.99597699e-02  3.06571443e-02\n",
      "    5.09577570e-03  1.87221002e-02  2.01695636e-02  4.15395834e-02\n",
      "    1.28433090e-02  5.97954635e-03  1.30867334e-02 -2.78082341e-02\n",
      "    2.48891152e-02  3.87531109e-02  6.73737191e-03 -9.77137592e-03\n",
      "   -4.29765657e-02  2.36162096e-02  1.82128400e-02 -2.28156932e-02\n",
      "    3.37565728e-02 -2.54657362e-02 -2.23405268e-02 -4.11574058e-02\n",
      "    4.64501865e-02 -1.60633698e-02 -1.92389300e-03 -4.53640670e-02\n",
      "   -4.32972498e-02 -1.72878169e-02 -3.07541508e-02  4.52694111e-03\n",
      "   -1.36441058e-02 -4.07247134e-02  2.07627378e-02 -2.13511661e-02]\n",
      "  [ 4.86431606e-02 -5.19695692e-02 -2.46172789e-02 -2.95909010e-02\n",
      "   -2.83480864e-02  6.29220670e-03  1.63709875e-02  1.57496106e-04\n",
      "    3.40735018e-02  7.74413347e-05  3.43121551e-02 -3.84196043e-02\n",
      "   -2.35492773e-02  2.85127554e-02 -3.16552818e-02 -1.08073000e-02\n",
      "   -3.88244763e-02 -1.14600849e-03  2.30145622e-02  4.13022973e-02\n",
      "    1.76746177e-03 -5.08970283e-02 -4.01302874e-02  1.16549265e-02\n",
      "   -1.11821378e-02  2.37596426e-02 -2.28581447e-02 -2.33152397e-02\n",
      "   -3.47695164e-02  1.18928179e-02 -3.59416381e-02  4.23572119e-03\n",
      "    7.77659472e-04  4.21420597e-02 -2.92235799e-02  3.53773162e-02\n",
      "   -5.15819676e-02 -1.78848859e-05  3.08536645e-02 -2.48512644e-02\n",
      "   -3.15888971e-02  2.15091426e-02  1.81848686e-02 -1.50000295e-02\n",
      "    3.44591215e-02 -4.56937589e-02 -2.42894031e-02  5.35657890e-02\n",
      "   -2.44036876e-02 -3.82275619e-02  1.90482084e-02  5.90201886e-03\n",
      "    1.05041191e-02  2.45351903e-02  1.03156762e-02  3.62097248e-02\n",
      "   -1.98933501e-02 -2.15351768e-02 -4.98245470e-02  1.81549985e-03\n",
      "    1.95592530e-02 -1.58324093e-02  1.12186875e-02  2.00506933e-02\n",
      "   -7.49164261e-03 -1.56808496e-02 -2.87320167e-02 -2.89484710e-02\n",
      "   -2.14090906e-02  1.87886413e-02  7.61801610e-04  4.56782198e-03\n",
      "   -1.86193790e-02 -8.91930889e-03 -2.44628675e-02 -3.62036601e-02\n",
      "   -2.51631904e-02  3.09791435e-02  2.31383764e-03  1.33656282e-02\n",
      "    4.46513183e-02  2.73046680e-02 -1.62797943e-02 -5.25099821e-02\n",
      "   -5.22785820e-03 -4.74010520e-02 -3.65665928e-02 -4.87782136e-02\n",
      "   -2.05028560e-02  1.72270406e-02 -2.78102588e-02  6.10805443e-03\n",
      "    3.79271917e-02 -3.33886556e-02 -2.99597699e-02  3.06571443e-02\n",
      "    5.09577570e-03  1.87221002e-02  2.01695636e-02  4.15395834e-02\n",
      "    1.28433090e-02  5.97954635e-03  1.30867334e-02 -2.78082341e-02\n",
      "    2.48891152e-02  3.87531109e-02  6.73737191e-03 -9.77137592e-03\n",
      "   -4.29765657e-02  2.36162096e-02  1.82128400e-02 -2.28156932e-02\n",
      "    3.37565728e-02 -2.54657362e-02 -2.23405268e-02 -4.11574058e-02\n",
      "    4.64501865e-02 -1.60633698e-02 -1.92389300e-03 -4.53640670e-02\n",
      "   -4.32972498e-02 -1.72878169e-02 -3.07541508e-02  4.52694111e-03\n",
      "   -1.36441058e-02 -4.07247134e-02  2.07627378e-02 -2.13511661e-02]]], shape=(1, 2, 128), dtype=float32)\n",
      "out shape: [[ 0.00968953  0.00838367 -0.00926169 ... -0.01185321 -0.01672084\n",
      "  -0.02621495]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits should be a matrix, got shape [10000] [Op:Multinomial]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-44e44f411d1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;31m#Generate a few sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;31m#         print(\" \".join([i2w[x] for x in sent]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-5da7d156b114>\u001b[0m in \u001b[0;36mgenerate_sent\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_score_of_histories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/random_ops.py\u001b[0m in \u001b[0;36mcategorical\u001b[0;34m(logits, num_samples, dtype, seed, name)\u001b[0m\n\u001b[1;32m    387\u001b[0m   \"\"\"\n\u001b[1;32m    388\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"categorical\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultinomial_categorical_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/random_ops.py\u001b[0m in \u001b[0;36mmultinomial_categorical_impl\u001b[0;34m(logits, num_samples, dtype, seed)\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m   return gen_random_ops.multinomial(\n\u001b[0;32m--> 397\u001b[0;31m       logits, num_samples, seed=seed1, seed2=seed2, output_dtype=dtype)\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36mmultinomial\u001b[0;34m(logits, num_samples, seed, seed2, output_dtype, name)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits should be a matrix, got shape [10000] [Op:Multinomial]"
     ]
    }
   ],
   "source": [
    "last_dev = 1e20\n",
    "best_dev = 1e20\n",
    "for ITER in range(5):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    # set the model to training mode\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for sent_id, sent in enumerate(train[:5]):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        train_loss += my_loss.numpy()\n",
    "        print(\"train_loss: {}\".format(train_loss))\n",
    "        train_words += len(sent)\n",
    "        if (sent_id+1) % 5000 == 0:\n",
    "            print(\"--finished %r sentences (word/sec=%.2f)\" % (sent_id+1, train_words/(time.time()-start)))\n",
    "    print(\"iter %r: train loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), train_words/(time.time()-start)))\n",
    "  \n",
    "     # Evaluate on dev set\n",
    "#     # set the model to evaluation mode\n",
    "#     model.eval()\n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for sent_id, sent in enumerate(dev[:5]):\n",
    "        my_loss = calc_valid_sent_loss(sent)\n",
    "        dev_loss += my_loss.numpy()\n",
    "        dev_words += len(sent)\n",
    "\n",
    "#     Keep track of the development accuracy and reduce the learning rate if it got worse\n",
    "        if last_dev < dev_loss:\n",
    "            print(optimizer._decayed_lr)\n",
    "    last_dev = dev_loss\n",
    "\n",
    "# Keep track of the best development accuracy, and save the model only if it's the best one\n",
    "#   if best_dev > dev_loss:\n",
    "#     torch.save(model, \"model.pt\")\n",
    "#     best_dev = dev_loss\n",
    "  \n",
    "#   # Save the model\n",
    "    print(\"iter %r: dev loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), dev_words/(time.time()-start)))\n",
    "  #Generate a few sentences\n",
    "    for _ in range(5):\n",
    "#         print(\" \".join([i2w[x] for x in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
