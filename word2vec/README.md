The word vectors trained in this model are only 10 dimensional due to constrained compute power. This is 
just to get the basic understanding of how word embeddings are trained.
